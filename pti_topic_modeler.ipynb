{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import MeCab\n",
    "STOPWORDS = []\n",
    "SKIP_WORD_CLASSES = [\"BOS/EOS\"]\n",
    "\n",
    "class MA:\n",
    "    def __init__(self):\n",
    "        self.mecab = MeCab.Tagger(\"-Ochasen -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "        self.stopwords = STOPWORDS\n",
    "        self.skip_word_classes  = SKIP_WORD_CLASSES\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        node = self.mecab.parseToNode(sentence)\n",
    "        res = []\n",
    "        while node:\n",
    "            cat1 = node.feature.split(\",\")[0]\n",
    "            if node.surface in STOPWORDS:\n",
    "                node = node.next\n",
    "                continue\n",
    "            if cat1 in SKIP_WORD_CLASSES:\n",
    "                node = node.next\n",
    "                continue\n",
    "            res.append(node.surface)\n",
    "            node = node.next\n",
    "        return \" \".join(res)\n",
    "\n",
    "import os\n",
    "dirname = \"/var/pti/scrape\"\n",
    "ma = MA()\n",
    "def load_file(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        try:\n",
    "            res = []\n",
    "            for line in f:\n",
    "                line = line.rstrip()\n",
    "                line = ma.parse(line)\n",
    "                if line is not None:\n",
    "                    res.append(line)\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    return \" \".join(res)\n",
    "\n",
    "def load_articles():\n",
    "    articles = []\n",
    "    idx2doc = []\n",
    "    for basename in sorted(os.listdir(dirname)):\n",
    "        basename = basename.rstrip()\n",
    "        if not basename.endswith(\".txt\"):\n",
    "            continue\n",
    "        filepath = os.path.join(dirname, basename)\n",
    "        article = load_file(filepath)\n",
    "        if article is None:\n",
    "            continue\n",
    "        idx2doc.append(basename.rstrip(\".txt\"))\n",
    "        articles.append(article)\n",
    "    return idx2doc, articles\n",
    "\n",
    "(idx2doc, dataset) = load_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2']\n",
      "['\\u3000 神奈川県小田原市 で 生活保護受給者 の 自立支援 を 担当 する 市職員 ら が 「 不正受給 は クズ だ 」 と の 趣旨 の 英文 が 書か れ た ジャンパー を 着 て い た 問題 で 、 職員 ら が 「 生活保護 悪 撲滅 チーム 」 を 示す 「 ＳＨＡＴ 」 の マーク を 袖 に 付け た 夏 用 の ポロシャツ を 製作 し 、 業務 で 着用 し て い た こと が ３ 日 、 市 へ の 取材 で 分かっ た 。 \\u3000 市 は 不適切 な マーク だ として 、 同日 以降 の 使用 を 禁止 し た 。 \\u3000 市 に よる と 、 ポロシャツ は 、 ジャンパー が 作ら れ た ２ ０ ０ ７ 年 の 翌年 夏 に 製作 さ れ 、 現在 も ２ ８ 人 の 職員 が 所持 し て いる 。 青 や 紺 など 数種類 あり 、 １ 着 ２ 千 円 前後 で 各 職員 が 個人 として 購入 し て い た 。  トランプ大統領 の 円安 誘導 批判 は 実力行使 を 伴わ ぬ 空砲 で あり 、 神通力 を 失う の は 時間 の 問題 だ と 野村証券 の 池田雄之輔 氏 は 分析 。 \\xa0 記事 の 全文 ドル円 相場 波乱 の 芽 は 、 中期的 に は トランプ政権 の 動向 より も 金融政策 に 潜ん で いる と バークレイズ 証券 の 門田 真一郎 氏 は 指摘 。 \\xa0 記事 の 全文 トムソン・ロイター は 世界最大 級 の 国際 マルチメディア 通信社 です 。 マーケット ニュース 、 ビジネスニュース 、 テクノロジー ニュース 、 外国為替 フォーラム 、 ワールド 、 ヘッドラインニュース 、 速報 ニュース 、 コラム 、 ブログ 、 国内 株式 、 海外 株式 、 外国為替 、 投資信託 情報 が 、 ロイター . co.jp サイト 、 ビデオ 、 モバイルサイト 、 インタラクティブ ・ テレビ プラットフォーム で ご 利用 いただけ ます 。 国内 株式 関連 の 情報 は 約 20分 遅れ 、 海外 株式 関連 の 情報 は 15分 以上 の 遅れ で 表示 し て い ます 。 為替 情報 は 10分 ごと に 更新 さ れ て おり 、 約 10分 前 の 相場 を 表示 し て い ます 。 日経平均株価 の 著 作 権 は 日本経済新聞社 に 帰属 し ます 。 本件 情報 を 無断 で 再 配信 する こと は 禁じ られ て おり ます 。 ロイター 日本語 ニュース は トムソン・ロイター が 提供 する 有料 金融 情報端末 「 トムソン・ロイター ・ アイコン 」 上 で 、 全て 優先 的 に ご覧 いただけ ます 。 リアルタイム で ニュース や 金融 デー タ を ご覧 いただく ため の 詳細 は こちら 。', '\\u3000 稲田朋美 防衛相 は ４ 日 午前 、 マティス 米 国防長官 と 防衛省 で 初めて 会談 し た 。 日本 による 米軍 駐留 経費 負担 は 議論 に なら ず 、 マティス 氏 は 会談 後 の 共同記者会見 で 「 日本 は コスト や 負担 の 共有 に関して モデル と なっ て き た 。 他国 が 見習う べき お手本 だ と 言える 」 と 評価 し た 。 双方 は 、 南シナ海 で の 中国 の 活動 に関し 「 安全保障 上 の 懸念 」 として 関与 を 強化 する 方針 で 合意 し た 。  \\u3000 北朝鮮 の 核 ・ ミサイル開発 は 、 日 米 両国 と 地域 へ の 重大 な 脅威 と の 認識 で 一致 。 日 米 韓 で 連携 対処 する こと を 確認 し た 。  \\u3000 在日米軍 駐留 経費 を 巡っ て は 、 トランプ 米大統領 が 選挙中 に 負担増 を 求める 考え を 示し て い た 。']\n"
     ]
    }
   ],
   "source": [
    "print(idx2doc)\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 2\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=1,\n",
    "                                max_features=n_features)\n",
    "tf = tf_vectorizer.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "             n_jobs=1, n_topics=2, perp_tol=0.1, random_state=0,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "import pickle\n",
    "pickle.dump(lda, open(\"lda.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "ロイター ます ニュース 情報 株式 職員 トムソン 日本 10分 ポロシャツ 関連 金融 会談 マティス ご覧 問題 あり 記事 いる 負担\n",
      "Topic #1:\n",
      "ます 共同記者会見 不適切 選挙中 株式 全文 職員 ニュース 情報 デー トランプ大統領 取材 トムソン 経費 相場 関連 作ら おり ジャンパー トランプ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9953175   0.0046825 ]\n",
      " [ 0.98606548  0.01393452]]\n"
     ]
    }
   ],
   "source": [
    "matrix = lda.transform(tf)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "for i, doc in enumerate(matrix):\n",
    "    result = []\n",
    "    for j, x in enumerate(doc):\n",
    "        result.append((j, x))\n",
    "    pickle.dump(result, open(\"/var/pti/topic/{}.pkl\".format(idx2doc[i]), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc\n",
      "Topic:0, prob:0.9953174983462851\n",
      "Topic:1, prob:0.004682501653714899\n",
      "doc\n",
      "Topic:0, prob:0.9860654820955974\n",
      "Topic:1, prob:0.013934517904402517\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(matrix):\n",
    "    topics = {}\n",
    "    print(\"doc\")\n",
    "    for v, x in enumerate(doc):\n",
    "        topics[v] = x\n",
    "    count = 0\n",
    "    for k, v  in sorted(topics.items(), key=lambda x:x[1], reverse=True):\n",
    "        print (\"Topic:{}, prob:{}\".format(k, v))\n",
    "        count += 1\n",
    "        if count >= 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
